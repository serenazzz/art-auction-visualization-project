---
title: "Art Project EDAV"
author: "Alexandra Sudomoeva, Elizabet Doliar, Serena Zheng, Basil"
date: "4/13/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lattice)
library(DAAG)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(grid)
```
# Introduction 

~**NEEDS INPUT**~
Explain why you chose this topic, and the questions you are interested in studying.

## Team Members's Contribution



# Data Description 

## Overview 

This dataset was provided by Arthena (https://arthena.com/). It was sourced from Sotheby’s historical auction data via the scraping of public web pages.  The raw dataset includes 22711 rows of data. Each row represents an individual lot from an auction (a lot could be an individual painting, a sculpture, or sometimes even a collection of works).  The raw dataset includes 25 columns of data.  Each column represents a feature related to either that specific lot, the artist of the lot, or the auction where the lot was sold.  Column definitions are listed below by category (Lot, Auction or Artist). For our analysis we also derive new columns from the original raw dataset. The details logic and thought process is described more in detail in the 'Data Quality Analysis'.

## Column Definitions: 

**Lot**
lot_id: a unique id for each lot
lot_title: the title of the lot. A lot can consist of multiple works (2-3 pieces of art in it). For our purposes we assume that 1 piece is 1 lot
estimate_low: need to follow up if this is a starting price? 
estimate_high: estimated high price by the auction house
hammer_price_bp: how much the lot was sold for at auction, plus buyers premium (a % fee).
currency: currency denomination for the price estimates and hammer price (USD, EUR, GBP, HKD) 
nth_in_auction: order that the lot was presented in at auction 
lot_number: number assigned to a lot for the given auction, different than nth_in_auction  
condition: description of the condition of the lot (messy text field - not used)
provenance: description of who owned the lot previously (messy text field - not used)
literature:  different publications that the lot was mentioned in (messy text field - not used)
external_image_url: link to the image (not used)

**Auction** 
auction_house_id: unique id for each auction house (for this dataset, all 1 = Sotheby’s)
auction_id: unique id for each auction
auc_title: title of the auction
number_of_lots: total number of lots in the auction
location: location where the auction was held
start_date: start date of the auction
end_date: end date of the auction (same as start date for this dataset)
auc_desc: description of the auction (messy text field - not used)
sale_id: unique auction sale id assigned by Sotheby’s

**Artist**
name: name of the artist
birth_year: artist’s approximate birth year  (messy text field - not used)
death_year: artist’s approximate death year  (messy text field - not used)



# Data Quality Analysis

## Preprocessing

In this part of the project, we will be exploring the quality of the data provided for the auction data. Before analyzing the overall quality, a simple preprocessing has been conducted in Python that included the following steps:

1. Added columns for normalized date (whole), year, month, and season.
2. 'Start Date' and 'End Date' are always the same within the dataset. Therefore, we decided to only user 'Start Date' for the time when the auction occured.
3. The "Title" column accepted a varity of title across different languages. We added a new column to show whether the piece is "untitles" while checking for the top 5 most common languages used: Italian, Dutch, English, French, Spanish, German.
4. Created a new column that would break the order of the auction into 10 tiles and group all pieces into those tiles by auction id
5. Added 'Average Estimate' column to reflect the aevrage price estimation between the high and the low
6. Converted all currency to USD to be able to compare pieces sold across different locations. We matched the exchange rate at the time of the sell to properly convert all transactions.

A complete logic with code can be found here: **INSERT LINK HERE**
 
## Data Quality Exploration
 
```{r}
art = read.csv("final_sothebys.csv", header=TRUE, na.strings=c("", NA))
#Save as a data frame
art_df <- as.data.frame(art)
#Dropping features with no interest (non-informative) in the analysis
drop <- c("X", "Unnamed..0", "provenance", "auction_house_id", "external_image_url", "literature")
art_df <- art_df[, !(names(art_df) %in% drop)]
```

Let us start by looking at all the missing values and if there are any general patterns. Since we are dealing with a large amount of rows, we will reduce repeated patters to one row using the visna function. 

```{r fig.height=3.5}
#install.packages("extracat")
library(extracat)
visna(art_df, sort= "b")
```

Looking at the output above, we can conclude that the overall state of the dataset is relatively good. The second most common pattern has no missing values while the two most "problematic" features appear to be death_year and birth_year.

One important observation we will need to be careful about during further analysis is a relatively significant amount of missing values under hammer_price_bp feature. We have explored qualitative reasons behind the missing values in that category with the provider of the data. After careful observation, we found out that the missing values are actually driven by two locations that migt have less strict regulations around data governance (Doha and Hong Kong). This observation is outlined in the graph below.   


```{r fig.width=4}
#install.packages("viridis")
#library(viridis)
art_location <- art_df %>% gather(attribute, value, -location) %>% mutate(missing = ifelse(is.na(value), "yes", "no"))
ggplot(art_location, aes(x = location, y = attribute, fill = missing)) +
  geom_tile(color = "white") + 
  ggtitle("Missing Values by Location") +
  xlab("Location") + ylab("Feature Name") +
  scale_fill_viridis(discrete=TRUE) +
  theme_bw()
```
This map allows us to see that some features' missing values like hammer_price_bp, condition, and auc_desc are actually only missing in certain location (typically in pairs of two). Therefore, we can speculate that this data is MAR (missing at random) depending on a location feature. 

Indeed, when looking at the table below, the overall percentage of missing values across difeerent locations varies quite significantly. Hong Kong is the top location with most missing values (8% of total). 

```{r}
art_location <- art_df %>% gather(attribute, value, -location)
percent_missing <- art_location %>% group_by(location) %>%
  summarise(num_na = sum(is.na(value)), total = n()) %>%
  mutate(percent_na = num_na/total)%>%
  arrange(-percent_na)
percent_missing
```

This leads to a logical question. Are there any other variables that could explain the missing data? Therefore, we also looked at simular graphs while grouping by year and month.

```{r fig.height = 3}
#install.packages("viridis")
#library(viridis)
art_year <- art_df %>% gather(attribute, value, -auc_year) %>% mutate(missing = ifelse(is.na(value), "yes", "no"))
year <- ggplot(art_year, aes(x = factor(auc_year), y = attribute, fill = missing)) +
  geom_tile(color = "white") + 
  ggtitle("Missing Values by Year") +
  xlab("year") + ylab("Feature Name") +
  scale_fill_viridis(discrete=TRUE) +
  theme_bw()
art_month <- art_df %>% gather(attribute, value, -auc_month) %>% mutate(missing = ifelse(is.na(value), "yes", "no"))
month <-ggplot(art_month, aes(x = factor(auc_month), y = attribute, fill =       
                                missing)) +
  geom_tile(color = "white") + 
  ggtitle("Missing Values by Month") +
  xlab("Month") + ylab("Feature Name") +
  scale_fill_viridis(discrete=TRUE) +
  theme_bw()
grid.arrange(year, month)
```

While month looks more or less equally distributed, there is an interesting pattern forming in the year view. It comes as no surprise that older years (2006-2009) would carry all of the missing values for features around description and condition. There also looks to be certain years (more like 3 year periods) with perfectly clean data: 2010-2012 and 2015-2017. 

Next, let us look at the exact percentages and values for the overall missing data.

```{r}
#install.packages("skimr")
library(skimr)
skimr::skim(art_df) %>% filter(stat =="missing") %>% arrange(desc(value)) %>% select(variable, value) %>% mutate(percent = value/nrow(art_df)) %>% filter (percent>0)
```

Looking at the table, it actually appears that the estimates (along with its corresponding values generated during pre-processing) are only missing a very small amount of data (less than 1%). Therefore, such a small error can be easily filtered out in future analysis. 


Nonetheless, hammer_price is missing nearly 20%. Besides what we saw when looking by location, our guess is that some auction ids are missing the hammer_price_bp in its entirety and hence the difference. It can be easily checked by looking at the aggregate table. 

```{r}
art_price <- art_df[, c("auction_id", "hammer_price_bp")]
percent_missing <- art_price %>% group_by(auction_id) %>%
  summarise(num_na = sum(is.na(hammer_price_bp)), total = n()) %>%
  mutate(percent_na = num_na/total)%>%
  arrange(-percent_na)
percent_missing %>% filter(percent_na>=0.4)
```

Looking at the data table, there is a significant number of auctions that are missing more than 10% of the price data (sometimes even 100%). Therefore, it must be that not only the locations but also the auction itself is a determining factor in missing hammer price value. 

Lastly, we would like to check for correlation between average hammer_price_bp and percentage of missing values (since it is the one feature that matters the most in our analysis and is most prime to such correlations). Could it be that very high/low lots are simply not reported and, therefore, are missing more?

```{r}
art_auction <- art_df[, c("auction_id", "hammer_price_bp")] %>% arrange(auction_id)
art_average <-art_auction %>% filter(!is.na(hammer_price_bp)) %>% 
  group_by(auction_id) %>% summarise(mean =mean(hammer_price_bp)) %>% arrange(auction_id)
percent_missing <- percent_missing %>% arrange(auction_id)
add_2 <- data.frame(auction_id=2, mean=0)
add_3 <- data.frame(auction_id=3, mean=0)
art_average <- rbind(art_average, add_2)
art_average <- rbind(art_average, add_3) %>% arrange(auction_id)
percent <- percent_missing[4]
average <- art_average[2]
auction_id <- percent_missing[1]
corr <- data.frame(auction_id=auction_id, percent=percent, average = average)
ggplot(corr, aes(average, percent)) + geom_point(col="blue") +ggtitle("Auction Average Hammer Price vs Percentage of Missing Values") 
```

There does not seem to be very strong correlation between the two variables.Just a subtle suggestion that auctions with smaller average price tend to have more NAs. Therefore, we will only consider location as the main determinant for missing values around hammer_price_bp. 

Before we move on, let us quickly follow up on the action items from the data quality analysis. 

1. filter out NAs for price estimates
2. take a note to exclude Doha and Hong Kong fro hammer_price_bp analysis

```{r}
art_final <- art_df %>% filter(!is.na(estimate_low))
```

# Main Analysis (Exploratory Data Analysis)

# Executive Summary

# Interactive Componenet